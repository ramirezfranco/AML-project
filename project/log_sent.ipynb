{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code based in HW1\n",
    "Credits: This assignment and notebook was originally created by Zewei Chu (zeweichu@uchicago.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as tud\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter, defaultdict\n",
    "import operator\n",
    "import os, math\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "# Feel free to define your own word_tokenizer instead of this naive \n",
    "# implementation. You may also use word_tokenize from nltk library \n",
    "# (from nltk import word_tokenize), which works better but slower. \n",
    "def word_tokenize(s):\n",
    "    return s.split()\n",
    "\n",
    "# set the random seeds so the experiments can be replicated exactly\n",
    "seed = 30255\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Global class labels.\n",
    "POS_LABEL = 'pos'\n",
    "NEG_LABEL = 'neg'     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file):\n",
    "    data = []\n",
    "    with open(data_file,'r',  encoding=\"utf8\") as fin:\n",
    "        for line in fin:\n",
    "            label, content = line.split(\",\", 1)\n",
    "            data.append((content.lower(), label))\n",
    "    return data\n",
    "data_dir = \"large_movie_review_dataset\"\n",
    "train_data = load_data(os.path.join(data_dir, \"train.txt\"))\n",
    "dev_data = load_data(os.path.join(data_dir, \"dev.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of TRAIN data 25000\n",
      "number of DEV data 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"number of TRAIN data\", len(train_data))\n",
    "print(\"number of DEV data\", len(dev_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined a generic model class as below. The model has 2 functions, train and classify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "class Model:\n",
    "    def __init__(self, data):\n",
    "        # Vocabulary is a set that stores every word seen in the \n",
    "        # training data\n",
    "        self.vocab = Counter([word for content, label in data \n",
    "                              for word in word_tokenize(content)]\n",
    "                            ).most_common(VOCAB_SIZE-1)\n",
    "        # word to index mapping\n",
    "        self.word_to_idx = {k[0]: v+1 for v, k in \n",
    "                            enumerate(self.vocab)}\n",
    "        # all the unknown words will be mapped to index 0\n",
    "        self.word_to_idx[\"UNK\"] = 0 \n",
    "        self.idx_to_word = {v:k for k, v in self.word_to_idx.items()}\n",
    "        self.label_to_idx = {POS_LABEL: 0, NEG_LABEL: 1}\n",
    "        self.idx_to_label = [POS_LABEL, NEG_LABEL]\n",
    "        self.vocab = set(self.word_to_idx.keys())\n",
    "        \n",
    "    def train_model(self, data):\n",
    "        '''\n",
    "        Train the model with the provided training data\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def classify(self, data):\n",
    "        '''\n",
    "        Classify the documents with the model\n",
    "        '''\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(tud.Dataset):\n",
    "    '''\n",
    "    PyTorch provides a common dataset interface. \n",
    "    See https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "    The dataset encodes documents into indices. \n",
    "    With the PyTorch dataloader, you can easily get batched data for \n",
    "    training and evaluation. \n",
    "    '''\n",
    "    def __init__(self, word_to_idx, data):\n",
    "        \n",
    "        self.data = data\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.label_to_idx = {POS_LABEL: 0, NEG_LABEL: 1}\n",
    "        self.vocab_size = VOCAB_SIZE\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = np.zeros(self.vocab_size)\n",
    "        \n",
    "        item = torch.FloatTensor(item)\n",
    "        # in training or tuning, we use both the document (review)\n",
    "        # and its corresponding label\n",
    "        if len(self.data[idx]) == 2: \n",
    "            for word in word_tokenize(self.data[idx][0]):\n",
    "                item[self.word_to_idx.get(word, 0)] += 1\n",
    "            label = self.label_to_idx[self.data[idx][1]]\n",
    "            #label = torch.LongTensor([label])\n",
    "            return item, label\n",
    "        else: # in testing, we only use the document without label\n",
    "            for word in word_tokenize(self.data[idx]):\n",
    "                item[self.word_to_idx.get(word, 0)] += 1\n",
    "            return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "class BoWLRClassifier(nn.Module, Model):\n",
    "    '''\n",
    "    Define your logistic regression model with bag of words features.\n",
    "    '''\n",
    "    def __init__(self, data):\n",
    "        nn.Module.__init__(self)\n",
    "        Model.__init__(self, data)\n",
    "        \n",
    "        '''\n",
    "        In this model initialization phase, write code to do the \n",
    "        following: \n",
    "        1. Define a linear layer to transform bag of words features \n",
    "           into 2 classes. \n",
    "        2. Define the loss function; use cross entropy loss (see\n",
    "            https://pytorch.org/docs/stable/nn.html?highlight=crossen#torch.nn.CrossEntropyLoss)\n",
    "        3. Define an optimizer for the model; choose the Adam optimizer,\n",
    "           which uses a version of the stochastic gradient descent \n",
    "           algorithm. (See https://pytorch.org/docs/stable/optim.html?highlight=sgd#torch.optim.Adam)\n",
    "        '''\n",
    "        self.mod= Model(data)\n",
    "        self.linear = nn.Linear(VOCAB_SIZE, len(self.mod.label_to_idx))\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.opt = optim.Adam(self.linear.parameters())\n",
    "        \n",
    "        \n",
    "    def forward(self, bow):\n",
    "        '''\n",
    "        Run the linear layer in the model for a single bag of words vector. \n",
    "        '''\n",
    "        # WRITE YOUR CODE HERE\n",
    "        # (You might be wondering why we don't explicitly have a\n",
    "        # softmax component in our model. It is included in something\n",
    "        # defined earlier. In what?)\n",
    "        return self.linear(bow)\n",
    "    \n",
    "    \n",
    "    def train_epoch(self, train_data):\n",
    "        '''\n",
    "        Train the model for one epoch with the training data\n",
    "        When training a model, you repeat the following procedure:\n",
    "        1. Get one batch of features and labels\n",
    "        2. Make a forward pass with the features to get predictions\n",
    "        3. Calculate the loss with the predictions and target labels\n",
    "        4. Run a backward pass from the loss function to get the gradients\n",
    "        5. Apply the optimizer step to update the model paramters\n",
    "        \n",
    "        For (1) you will have to understand how the PyTorch dataloader\n",
    "        functions.\n",
    "        '''\n",
    "        # WRITE YOUR CODE HERE\n",
    "        \n",
    "        #mod = Model(train_data)\n",
    "        #w2i = Model(train_data).word_to_idx\n",
    "        #batches = TextClassificationDataset(w2i, train_data)\n",
    "        for i in range(len(train_data)):\n",
    "            self.linear.zero_grad()\n",
    "            x, y = train_data[i]\n",
    "            pred = self.forward(x)\n",
    "            loss = self.loss(pred.view(1, -1), torch.LongTensor([y]))\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            if i%5000==0:\n",
    "                print('still working: {}% of completion of this epoch. Loss = {}'.format((i/len(train_data))*100,loss))\n",
    "        \n",
    "    \n",
    "    def classify(self, docs):\n",
    "        '''\n",
    "        This function classifies documents into their categories. \n",
    "        docs are documents without labels.\n",
    "        '''\n",
    "        # WRITE YOUR CODE HERE\n",
    "        probs = self.forward(docs)\n",
    "        cl = 1\n",
    "        if probs[0] >= probs[1]:\n",
    "            cl = 0\n",
    "        return cl\n",
    "                \n",
    "    def evaluate_classifier_accuracy(self, data):\n",
    "        '''\n",
    "        This function evaluates the data with the current model. \n",
    "        data contains both documents and labels. \n",
    "        It calls classify() to make predictions, \n",
    "        and compares with the correct labels to return \n",
    "        the model accuracy on \"data\". \n",
    "        '''\n",
    "        # WRITE YOUR CODE HERE\n",
    "        correct = 0\n",
    "        for i in range(len(data)):\n",
    "            bow, label = data[i]\n",
    "            pred = self.classify(bow)\n",
    "            if pred == label:\n",
    "                correct += 1\n",
    "        return correct/len(data)\n",
    "            \n",
    "    \n",
    "    def train_model(self, train_data, dev_data):\n",
    "        \"\"\"\n",
    "        This function processes the entire training set for multiple epochs.\n",
    "        After each training epoch, evaluate your model on the DEV set. \n",
    "        Save the best performing model on the DEV set to best_model\n",
    "        \"\"\"  \n",
    "        num_epoch = 5\n",
    "        batches_train=TextClassificationDataset(self.mod.word_to_idx, train_data)\n",
    "        batches_dev=TextClassificationDataset(self.mod.word_to_idx, dev_data)\n",
    "        best_acc = 0\n",
    "        for e in range(num_epoch):\n",
    "            self.train_epoch(batches_train)\n",
    "            acc = self.evaluate_classifier_accuracy(batches_dev)\n",
    "            if acc < best_acc:\n",
    "                return 0\n",
    "            best_acc = acc\n",
    "            print('The accuracy after epoch {} is {}%'.format(e+1,round(acc*100, 2)))\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr_model = BoWLRClassifier(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "still working: 0.0% of completion of this epoch. Loss = 0.6903829574584961\n",
      "still working: 20.0% of completion of this epoch. Loss = 0.07361388206481934\n",
      "still working: 40.0% of completion of this epoch. Loss = 0.0046694278717041016\n",
      "still working: 60.0% of completion of this epoch. Loss = 1.9550323486328125e-05\n",
      "still working: 80.0% of completion of this epoch. Loss = 0.007635593414306641\n",
      "The accuracy after epoch 1 is 86.3%\n",
      "still working: 0.0% of completion of this epoch. Loss = 8.726119995117188e-05\n",
      "still working: 20.0% of completion of this epoch. Loss = 0.014612436294555664\n",
      "still working: 40.0% of completion of this epoch. Loss = 1.621246337890625e-05\n",
      "still working: 60.0% of completion of this epoch. Loss = 3.7670135498046875e-05\n",
      "still working: 80.0% of completion of this epoch. Loss = 0.0024690628051757812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.train_model(train_data, dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWLRClassifierT(nn.Module, Model):\n",
    "    '''\n",
    "    Define your logistic regression model with bag of words features.\n",
    "    '''\n",
    "    def __init__(self, data, EPOCHs, op, lr, weight_decay):\n",
    "        nn.Module.__init__(self)\n",
    "        Model.__init__(self, data)\n",
    "        \n",
    "        '''\n",
    "        In this model initialization phase, write code to do the \n",
    "        following: \n",
    "        1. Define a linear layer to transform bag of words features \n",
    "           into 2 classes. \n",
    "        2. Define the loss function; use cross entropy loss (see\n",
    "            https://pytorch.org/docs/stable/nn.html?highlight=crossen#torch.nn.CrossEntropyLoss)\n",
    "        3. Define an optimizer for the model; choose the Adam optimizer,\n",
    "           which uses a version of the stochastic gradient descent \n",
    "           algorithm. (See https://pytorch.org/docs/stable/optim.html?highlight=sgd#torch.optim.Adam)\n",
    "        '''\n",
    "        self.epochs = EPOCHs\n",
    "        self.mod= Model(data)\n",
    "        self.linear = nn.Linear(VOCAB_SIZE, len(self.mod.label_to_idx))\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.opt = op(self.linear.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        \n",
    "    def forward(self, bow):\n",
    "        '''\n",
    "        Run the linear layer in the model for a single bag of words vector. \n",
    "        '''\n",
    "        # WRITE YOUR CODE HERE\n",
    "        # (You might be wondering why we don't explicitly have a\n",
    "        # softmax component in our model. It is included in something\n",
    "        # defined earlier. In what?)\n",
    "        return self.linear(bow)\n",
    "    \n",
    "    \n",
    "    def train_epoch(self, train_data):\n",
    "        '''\n",
    "        Train the model for one epoch with the training data\n",
    "        When training a model, you repeat the following procedure:\n",
    "        1. Get one batch of features and labels\n",
    "        2. Make a forward pass with the features to get predictions\n",
    "        3. Calculate the loss with the predictions and target labels\n",
    "        4. Run a backward pass from the loss function to get the gradients\n",
    "        5. Apply the optimizer step to update the model paramters\n",
    "        \n",
    "        For (1) you will have to understand how the PyTorch dataloader\n",
    "        functions.\n",
    "        '''\n",
    "        # WRITE YOUR CODE HERE\n",
    "        sum_loss = 0\n",
    "        for i in range(len(train_data)):\n",
    "            self.linear.zero_grad()\n",
    "            x, y = train_data[i]\n",
    "            pred = self.forward(x)\n",
    "            loss = self.loss(pred.view(1, -1), torch.LongTensor([y]))\n",
    "            sum_loss += loss\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            if i%2500==0 and i>0:\n",
    "                print('{}% of completion of this epoch. Average Loss = {}'.format((i/len(train_data))*100,sum_loss/2500))\n",
    "                sum_loss = 0\n",
    "        \n",
    "    \n",
    "    def classify(self, docs):\n",
    "        '''\n",
    "        This function classifies documents into their categories. \n",
    "        docs are documents without labels.\n",
    "        '''\n",
    "        # WRITE YOUR CODE HERE\n",
    "        probs = F.log_softmax(self.forward(docs).view(1, -1), dim=1)[0]\n",
    "        cl = 1\n",
    "        if probs[0] >= probs[1]:\n",
    "            cl = 0\n",
    "        return cl\n",
    "                \n",
    "    def evaluate_classifier_accuracy(self, data):\n",
    "        '''\n",
    "        This function evaluates the data with the current model. \n",
    "        data contains both documents and labels. \n",
    "        It calls classify() to make predictions, \n",
    "        and compares with the correct labels to return \n",
    "        the model accuracy on \"data\". \n",
    "        '''\n",
    "        # WRITE YOUR CODE HERE\n",
    "        correct = 0\n",
    "        for i in range(len(data)):\n",
    "            bow, label = data[i]\n",
    "            pred = self.classify(bow)\n",
    "            if pred == label:\n",
    "                correct += 1\n",
    "        return correct/len(data)\n",
    "            \n",
    "    \n",
    "    def train_model(self, train_data, dev_data):\n",
    "        \"\"\"\n",
    "        This function processes the entire training set for multiple epochs.\n",
    "        After each training epoch, evaluate your model on the DEV set. \n",
    "        Save the best performing model on the DEV set to best_model\n",
    "        \"\"\"  \n",
    "        batches_train=TextClassificationDataset(self.mod.word_to_idx, train_data)\n",
    "        batches_dev=TextClassificationDataset(self.mod.word_to_idx, dev_data)\n",
    "        best_acc = 0\n",
    "        for e in range(self.epochs):\n",
    "            self.train_epoch(batches_train)\n",
    "            acc = self.evaluate_classifier_accuracy(batches_dev)\n",
    "            if acc < best_acc or acc < 0.6:\n",
    "                print('Model stopped')\n",
    "                return 0\n",
    "            best_acc = acc\n",
    "            print('The accuracy after epoch {} is {}%'.format(e+1,round(acc*100, 2)))\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0% of completion of this epoch. Average Loss = 0.7018959522247314\n",
      "20.0% of completion of this epoch. Average Loss = 0.6727177500724792\n",
      "30.0% of completion of this epoch. Average Loss = 0.6563047766685486\n",
      "40.0% of completion of this epoch. Average Loss = 0.6330961585044861\n",
      "50.0% of completion of this epoch. Average Loss = 0.621982753276825\n",
      "60.0% of completion of this epoch. Average Loss = 0.6039065718650818\n",
      "70.0% of completion of this epoch. Average Loss = 0.588162362575531\n",
      "80.0% of completion of this epoch. Average Loss = 0.582866370677948\n",
      "90.0% of completion of this epoch. Average Loss = 0.569302499294281\n",
      "The accuracy after epoch 1 is 79.48%\n",
      "10.0% of completion of this epoch. Average Loss = 0.5480149388313293\n",
      "20.0% of completion of this epoch. Average Loss = 0.5391193628311157\n",
      "30.0% of completion of this epoch. Average Loss = 0.5304416418075562\n",
      "40.0% of completion of this epoch. Average Loss = 0.5207783579826355\n",
      "50.0% of completion of this epoch. Average Loss = 0.5194603204727173\n",
      "60.0% of completion of this epoch. Average Loss = 0.5064206123352051\n",
      "70.0% of completion of this epoch. Average Loss = 0.4953025281429291\n",
      "80.0% of completion of this epoch. Average Loss = 0.496279239654541\n",
      "90.0% of completion of this epoch. Average Loss = 0.48841342329978943\n",
      "The accuracy after epoch 2 is 83.18%\n",
      "10.0% of completion of this epoch. Average Loss = 0.4773321747779846\n",
      "20.0% of completion of this epoch. Average Loss = 0.4714541435241699\n",
      "30.0% of completion of this epoch. Average Loss = 0.46487948298454285\n",
      "40.0% of completion of this epoch. Average Loss = 0.4610220193862915\n",
      "50.0% of completion of this epoch. Average Loss = 0.46212685108184814\n",
      "60.0% of completion of this epoch. Average Loss = 0.4520946145057678\n",
      "70.0% of completion of this epoch. Average Loss = 0.4420537054538727\n",
      "80.0% of completion of this epoch. Average Loss = 0.44545894861221313\n",
      "90.0% of completion of this epoch. Average Loss = 0.44094693660736084\n",
      "The accuracy after epoch 3 is 84.62%\n",
      "10.0% of completion of this epoch. Average Loss = 0.43395569920539856\n",
      "20.0% of completion of this epoch. Average Loss = 0.4299342632293701\n",
      "30.0% of completion of this epoch. Average Loss = 0.4243401885032654\n",
      "40.0% of completion of this epoch. Average Loss = 0.42285123467445374\n",
      "50.0% of completion of this epoch. Average Loss = 0.42504119873046875\n",
      "60.0% of completion of this epoch. Average Loss = 0.4167104959487915\n",
      "70.0% of completion of this epoch. Average Loss = 0.40707287192344666\n",
      "80.0% of completion of this epoch. Average Loss = 0.41159605979919434\n",
      "90.0% of completion of this epoch. Average Loss = 0.4094673693180084\n",
      "The accuracy after epoch 4 is 85.36%\n",
      "10.0% of completion of this epoch. Average Loss = 0.40414461493492126\n",
      "20.0% of completion of this epoch. Average Loss = 0.4015137553215027\n",
      "30.0% of completion of this epoch. Average Loss = 0.39648085832595825\n",
      "40.0% of completion of this epoch. Average Loss = 0.3958626687526703\n",
      "50.0% of completion of this epoch. Average Loss = 0.39874714612960815\n",
      "60.0% of completion of this epoch. Average Loss = 0.3914155960083008\n",
      "70.0% of completion of this epoch. Average Loss = 0.38202258944511414\n",
      "80.0% of completion of this epoch. Average Loss = 0.38700753450393677\n",
      "90.0% of completion of this epoch. Average Loss = 0.3867017328739166\n",
      "The accuracy after epoch 5 is 86.02%\n",
      "10.0% of completion of this epoch. Average Loss = 0.38204318284988403\n",
      "20.0% of completion of this epoch. Average Loss = 0.3805220127105713\n",
      "30.0% of completion of this epoch. Average Loss = 0.3758966326713562\n",
      "40.0% of completion of this epoch. Average Loss = 0.37546440958976746\n",
      "50.0% of completion of this epoch. Average Loss = 0.3788611590862274\n",
      "60.0% of completion of this epoch. Average Loss = 0.3721499443054199\n",
      "70.0% of completion of this epoch. Average Loss = 0.36297276616096497\n",
      "80.0% of completion of this epoch. Average Loss = 0.368071049451828\n",
      "90.0% of completion of this epoch. Average Loss = 0.36920469999313354\n",
      "The accuracy after epoch 6 is 86.16%\n",
      "10.0% of completion of this epoch. Average Loss = 0.3647767901420593\n",
      "20.0% of completion of this epoch. Average Loss = 0.36415722966194153\n",
      "30.0% of completion of this epoch. Average Loss = 0.3598921298980713\n",
      "40.0% of completion of this epoch. Average Loss = 0.3593120872974396\n",
      "50.0% of completion of this epoch. Average Loss = 0.3631018400192261\n",
      "60.0% of completion of this epoch. Average Loss = 0.3568003177642822\n",
      "70.0% of completion of this epoch. Average Loss = 0.34783995151519775\n",
      "80.0% of completion of this epoch. Average Loss = 0.35286539793014526\n",
      "90.0% of completion of this epoch. Average Loss = 0.35515493154525757\n",
      "The accuracy after epoch 7 is 86.38%\n",
      "10.0% of completion of this epoch. Average Loss = 0.350765585899353\n",
      "20.0% of completion of this epoch. Average Loss = 0.35088467597961426\n",
      "30.0% of completion of this epoch. Average Loss = 0.34697484970092773\n",
      "40.0% of completion of this epoch. Average Loss = 0.3460811972618103\n",
      "50.0% of completion of this epoch. Average Loss = 0.35017722845077515\n",
      "60.0% of completion of this epoch. Average Loss = 0.3441562056541443\n",
      "70.0% of completion of this epoch. Average Loss = 0.33542123436927795\n",
      "80.0% of completion of this epoch. Average Loss = 0.34027576446533203\n",
      "90.0% of completion of this epoch. Average Loss = 0.3435032069683075\n",
      "The accuracy after epoch 8 is 86.64%\n",
      "10.0% of completion of this epoch. Average Loss = 0.33907046914100647\n",
      "20.0% of completion of this epoch. Average Loss = 0.3397972583770752\n",
      "30.0% of completion of this epoch. Average Loss = 0.33625030517578125\n",
      "40.0% of completion of this epoch. Average Loss = 0.33496060967445374\n",
      "50.0% of completion of this epoch. Average Loss = 0.3392927348613739\n",
      "60.0% of completion of this epoch. Average Loss = 0.3334752321243286\n",
      "70.0% of completion of this epoch. Average Loss = 0.3249715268611908\n",
      "80.0% of completion of this epoch. Average Loss = 0.3296055793762207\n",
      "90.0% of completion of this epoch. Average Loss = 0.3335995376110077\n",
      "The accuracy after epoch 9 is 86.9%\n",
      "10.0% of completion of this epoch. Average Loss = 0.3290922939777374\n",
      "20.0% of completion of this epoch. Average Loss = 0.330320805311203\n",
      "30.0% of completion of this epoch. Average Loss = 0.3271499276161194\n",
      "40.0% of completion of this epoch. Average Loss = 0.32542580366134644\n",
      "50.0% of completion of this epoch. Average Loss = 0.3299356698989868\n",
      "60.0% of completion of this epoch. Average Loss = 0.3242713212966919\n",
      "70.0% of completion of this epoch. Average Loss = 0.3160037696361542\n",
      "80.0% of completion of this epoch. Average Loss = 0.32039347290992737\n",
      "90.0% of completion of this epoch. Average Loss = 0.3250177800655365\n",
      "The accuracy after epoch 10 is 87.1%\n",
      "10.0% of completion of this epoch. Average Loss = 0.32043102383613586\n",
      "20.0% of completion of this epoch. Average Loss = 0.32207435369491577\n",
      "30.0% of completion of this epoch. Average Loss = 0.3192882239818573\n",
      "40.0% of completion of this epoch. Average Loss = 0.3171178102493286\n",
      "50.0% of completion of this epoch. Average Loss = 0.32175976037979126\n",
      "60.0% of completion of this epoch. Average Loss = 0.3162148594856262\n",
      "70.0% of completion of this epoch. Average Loss = 0.3081856071949005\n",
      "80.0% of completion of this epoch. Average Loss = 0.3123215138912201\n",
      "90.0% of completion of this epoch. Average Loss = 0.3174668848514557\n",
      "The accuracy after epoch 11 is 87.12%\n",
      "10.0% of completion of this epoch. Average Loss = 0.31280723214149475\n",
      "20.0% of completion of this epoch. Average Loss = 0.3147960305213928\n",
      "30.0% of completion of this epoch. Average Loss = 0.3124004006385803\n",
      "40.0% of completion of this epoch. Average Loss = 0.30978432297706604\n",
      "50.0% of completion of this epoch. Average Loss = 0.31451845169067383\n",
      "60.0% of completion of this epoch. Average Loss = 0.3090706169605255\n",
      "70.0% of completion of this epoch. Average Loss = 0.3012821674346924\n",
      "80.0% of completion of this epoch. Average Loss = 0.30516302585601807\n",
      "90.0% of completion of this epoch. Average Loss = 0.310742050409317\n",
      "The accuracy after epoch 12 is 87.28%\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "epoch_numbers = [12]\n",
    "optimizer = [optim.Adam]#[optim.SGD, optim.Adam]\n",
    "learning_rates = [0.00001]\n",
    "decays = [0]\n",
    "best_accuracy = 0\n",
    "for e in epoch_numbers:\n",
    "    for o in optimizer:\n",
    "        for l in learning_rates:\n",
    "            for d in decays:\n",
    "                model_def = '{} epochs, using {} optimizer, lr={} and weight_decay = {}'.format(e, o, l, d)\n",
    "                lr_model_t = BoWLRClassifierT(data=train_data, EPOCHs=e, op=o, lr=l, weight_decay=d)\n",
    "                accuracy = lr_model_t.train_model(train_data, dev_data)\n",
    "                results.append([model_def, accuracy])\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_model = copy.deepcopy(lr_model_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8728"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoWLRClassifierT(\n",
       "  (linear): Linear(in_features=5000, out_features=2, bias=True)\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "beach = util.open_json('beach_to_process.json')\n",
    "nature = util.open_json('nature_to_process.json')\n",
    "culture = util.open_json('culture_to_process.json')\n",
    "shopping = util.open_json('shopping_to_process.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sent(segments_dict, best_model):\n",
    "    results = {}\n",
    "    cities = segments_dict.keys()\n",
    "    for city in cities:\n",
    "        segments = segments_dict[city]\n",
    "        vocab = Counter([v for sent in segments for v in word_tokenize(sent)])\n",
    "        seg_word_to_idx = {k: v+1 for v, k in enumerate(vocab)}\n",
    "        segmens_rev = TextClassificationDataset(seg_word_to_idx, segments)\n",
    "        city_sents = [best_model.classify(r) for r in segmens_rev]\n",
    "        if len(city_sents)>10:\n",
    "            ratios = {'positive':round(len([j for j in city_sents if j ==0]),2)/len(city_sents),\n",
    "                      'negative':round(len([j for j in city_sents if j ==1]),2)/len(city_sents)}\n",
    "            results[city]=ratios\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopping_sent = log_sent(shopping, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hermosillo</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chihuahua</th>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancun</th>\n",
       "      <td>0.513064</td>\n",
       "      <td>0.486936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ciudad juarez</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zapopan</th>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.576923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tijuana</th>\n",
       "      <td>0.393443</td>\n",
       "      <td>0.606557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leon</th>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>puerto penasco</th>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                positive  negative\n",
       "hermosillo      0.545455  0.454545\n",
       "chihuahua       0.538462  0.461538\n",
       "cancun          0.513064  0.486936\n",
       "ciudad juarez   0.444444  0.555556\n",
       "zapopan         0.423077  0.576923\n",
       "tijuana         0.393443  0.606557\n",
       "leon            0.384615  0.615385\n",
       "puerto penasco  0.153846  0.846154"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(shopping_sent, orient='index').sort_values(by='positive', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>monterrey</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oaxaca</th>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.408163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valle de bravo</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ciudad valles</th>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>palenque</th>\n",
       "      <td>0.394667</td>\n",
       "      <td>0.605333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comitan</th>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                positive  negative\n",
       "monterrey       0.666667  0.333333\n",
       "oaxaca          0.591837  0.408163\n",
       "valle de bravo  0.500000  0.500000\n",
       "ciudad valles   0.421053  0.578947\n",
       "palenque        0.394667  0.605333\n",
       "comitan         0.277778  0.722222"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nature_sent = log_sent(nature, best_model)\n",
    "pd.DataFrame.from_dict(nature_sent, orient='index').sort_values(by='positive', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>saltillo</th>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.391304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xalapa</th>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.392157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cuernavaca</th>\n",
       "      <td>0.474359</td>\n",
       "      <td>0.525641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monterrey</th>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.543210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chihuahua</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ciudad juarez</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aguascalientes</th>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oaxaca</th>\n",
       "      <td>0.370474</td>\n",
       "      <td>0.629526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexico city</th>\n",
       "      <td>0.358798</td>\n",
       "      <td>0.641202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>san cristobal de las casas</th>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.785714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            positive  negative\n",
       "saltillo                    0.608696  0.391304\n",
       "xalapa                      0.607843  0.392157\n",
       "cuernavaca                  0.474359  0.525641\n",
       "monterrey                   0.456790  0.543210\n",
       "chihuahua                   0.400000  0.600000\n",
       "ciudad juarez               0.400000  0.600000\n",
       "aguascalientes              0.391304  0.608696\n",
       "oaxaca                      0.370474  0.629526\n",
       "mexico city                 0.358798  0.641202\n",
       "san cristobal de las casas  0.214286  0.785714"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "culture_sent = log_sent(culture, best_model)\n",
    "pd.DataFrame.from_dict(culture_sent, orient='index').sort_values(by='positive', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>todos santos</th>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.284553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sayulita</th>\n",
       "      <td>0.654485</td>\n",
       "      <td>0.345515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manzanillo</th>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.377778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zihuatanejo de azueta</th>\n",
       "      <td>0.605042</td>\n",
       "      <td>0.394958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cabo san lucas</th>\n",
       "      <td>0.573472</td>\n",
       "      <td>0.426528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zihuatanejo</th>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>puerto escondido</th>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.515152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rosarito</th>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.523810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huatulco</th>\n",
       "      <td>0.427119</td>\n",
       "      <td>0.572881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>punta de mita</th>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       positive  negative\n",
       "todos santos           0.715447  0.284553\n",
       "sayulita               0.654485  0.345515\n",
       "manzanillo             0.622222  0.377778\n",
       "zihuatanejo de azueta  0.605042  0.394958\n",
       "cabo san lucas         0.573472  0.426528\n",
       "zihuatanejo            0.525000  0.475000\n",
       "puerto escondido       0.484848  0.515152\n",
       "rosarito               0.476190  0.523810\n",
       "huatulco               0.427119  0.572881\n",
       "punta de mita          0.266667  0.733333"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beach_sent = log_sent(beach, best_model)\n",
    "pd.DataFrame.from_dict(beach_sent, orient='index').sort_values(by='positive', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
